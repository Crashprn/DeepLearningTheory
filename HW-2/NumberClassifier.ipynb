{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''*** Importing Libraries ***'''\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Utility Functions'''\n",
    "\n",
    "def to_numpy(x: t.Tensor) -> np.ndarray:\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def vectorized_result(j: int) -> np.ndarray:\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_mnist():\n",
    "    with gzip.open('./data/mnist.pkl.gz', 'rb') as f:\n",
    "        tr_d, va_d, te_d = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, T: t.Tensor) -> t.Tensor:\n",
    "        return self.classifier(T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberClassifier:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr = 1e-3):\n",
    "        self.device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "        self.model = ClassifierNetwork(input_size, hidden_size, output_size).to(self.device)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = t.optim.SGD(self.model.parameters(), lr = lr)\n",
    "    \n",
    "    def train(self, training_data, epochs = 100, batch_size = 32, test_data = None):\n",
    "        epoch_loss = []\n",
    "        training_data = list(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            mini_batch_loss = []\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[i:i+batch_size] for i in range(0, len(training_data), batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                x = [x[0] for x in mini_batch]\n",
    "                y = [x[1] for x in mini_batch]\n",
    "\n",
    "                x = t.tensor(x).squeeze().to(self.device)\n",
    "                y = t.tensor(y).squeeze().to(self.device)\n",
    "\n",
    "                mini_batch_loss.append(self.update(x, y))\n",
    "\n",
    "            epoch_loss.append(np.mean(mini_batch_loss))\n",
    "            print(f'---------------Epoch {epoch}------------------')\n",
    "            print(f'Loss: {epoch_loss[-1]}')\n",
    "            if test_data:\n",
    "                x_test = [x[0] for x in test_data]\n",
    "                y_test = [x[1] for x in test_data]\n",
    "                x_test = t.tensor(x_test).squeeze().to(self.device)\n",
    "                y_test = t.tensor(y_test).squeeze()\n",
    "                print(f'Test Accuracy: {self.evaluate(x_test, y_test)} / {len(test_data)}')\n",
    "\n",
    "        return epoch_loss\n",
    "    \n",
    "    def update(self, x: t.Tensor, y: t.Tensor):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.model(x).to(t.double)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return to_numpy(loss)\n",
    "\n",
    "    def evaluate(self, x: t.Tensor, y: t.Tensor):\n",
    "        pred = self.predict(x)\n",
    "        picks = np.argmax(pred, axis = 1) \n",
    "        logical = [int(x == y) for x,y in zip(picks, y)]\n",
    "        return sum(logical)\n",
    "    \n",
    "    def predict(self, x: t.Tensor):\n",
    "        return to_numpy(self.model(x))\n",
    "\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4219/599988226.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  x = t.tensor(x).squeeze().to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Epoch 0------------------\n",
      "Loss: 0.09035380888344269\n",
      "Test Accuracy: 2937 / 10000\n",
      "---------------Epoch 1------------------\n",
      "Loss: 0.08781168427888705\n",
      "Test Accuracy: 2652 / 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_data, validation_data, test_data \u001b[39m=\u001b[39m load_mnist()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m classifier \u001b[39m=\u001b[39m NumberClassifier(\u001b[39m784\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m10\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mtrain(training_data, test_data\u001b[39m=\u001b[39;49mtest_data)\n",
      "\u001b[1;32m/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m mini_batch]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m y \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m mini_batch]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mtensor(x)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m y \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor(y)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/codygrogan/Classes/DeepLearningTheory/HW-2/NumberClassifier.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m mini_batch_loss\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(x, y))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_mnist()\n",
    "\n",
    "classifier = NumberClassifier(784, 30, 10, lr=1)\n",
    "\n",
    "loss = classifier.train(training_data, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
