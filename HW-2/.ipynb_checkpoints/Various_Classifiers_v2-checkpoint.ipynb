{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ".....IMPORTANT USAGE INSTRUCTIONS........\n",
    "\n",
    "##### IF USING CHPC - UTAH #####\n",
    "\n",
    "1. Download this Jupyter Notebook to a local location on your Computer\n",
    "2. Go to https://ondemand.chpc.utah.edu and sign in using your uNID and Password.\n",
    "3. At the Top of the Page, notice the Menu \"Interactive Apps\". Click and Choose \"Jupyter Notebook on Notchpeak\"\n",
    "4. A form will open, enter all details, and then Launch a Jupyter Notebook. It will take a minute.\n",
    "5. Click on \"Connect to Jupyter\"\n",
    "6. Once Jupyter Launches. On Top Right Notice \"Upload Button\". Use this to Upload this Notebook.\n",
    "7. The Notebook will be uploaded. Finish writing the Code whereever specified.\n",
    "8. Run each Block of Code and then finally download the Jupyter Notebook by going to File >> Download as >>\n",
    "\n",
    "\n",
    "##### IF USING GOOGLE COLAB #####\n",
    "\n",
    "1. Download this Jupyter Notebook to a local location on your Computer\n",
    "2. Go to https://colab.research.google.com/ and sign in using your Google Account - So that your work is saved in\n",
    "   your Google Drive permanently.\n",
    "3. Go to File >> Upload Notebook.\n",
    "4. Finish writing the Code whereever specified.\n",
    "5. Run each Block of Code ad then finally download the Jupyter Notebook by going to File >> Download .ipynb\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ".....IMPORTANT SUBMISSION INSTRUCTIONS........\n",
    "\n",
    "Once everything runs successfully, download the jupyter notebook and attach that to your submission in Canvas. \n",
    "During evaluation, I will run your Jupyter Notebook to verify that everything is running as expected.\n",
    "\n",
    "Do not forget to include your main results and plots in your latex file (with other homework questions) \n",
    "before submission.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Problem 1 - Refer to your own code from HW - 1\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "(i) You are implementing a ridge regression. Recall what purpose the penalty term serves.\n",
    "(ii) By design, we expect your estimates for the 9th degree polynomial to shrink when compared to the estimates\n",
    "generated in HW - 1. If you are using the same seed as HW - 1, you can observe the shrinkage yourself.\n",
    "(iii) Also, notice, what happens when you keep changing your shrinkage parameter.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Problem 2.1 - Code has been given to you. Make sure you understand every line of the import step and data structuring\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Matplotlib Settings with ggplot Theme\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data Import and Preprocessing\n",
    "\n",
    "'''\n",
    "Note : The dataset data_seed.dat can be downloaded from Canvas or github. In this notebook, I have placed this file\n",
    "       directly under my home directory indicated by ~/.\n",
    "       \n",
    "       In CHPC - Your home directory is /uufs/cpc.utah.edu/common/home/<uNID>\n",
    "       \n",
    "       In Google Colab - On the Left Hand Menu, Click on Files and then Upload this data file.\n",
    "\n",
    "'''\n",
    "\n",
    "#Import Data File into Pandas - 210 Rows and 8 Columns. The dataset has no columns\n",
    "df = pd.read_csv('~/data_seed.dat', sep='\\s+', header=None, skiprows=0)\n",
    "\n",
    "#Add Column Names\n",
    "df.columns = ['A', 'P', 'C', 'L_Kern','W_Kern', 'Asy_Coeff','L_Kern_Grv','Y']\n",
    "\n",
    "#Scale X Columns -  The idea is to Scale each column in the dataset using built in Standard Scaler\n",
    "cols_to_norm = ['A', 'P', 'C', 'L_Kern','W_Kern', 'Asy_Coeff','L_Kern_Grv']\n",
    "df[cols_to_norm] = StandardScaler().fit_transform(df[cols_to_norm])\n",
    "\n",
    "#Shuffle the Dataset and then split into 5 separate dataframes to be used later for CV\n",
    "seeds = shuffle(df)\n",
    "\n",
    "#Split shuffled data into 5 data frames of size 42 each.\n",
    "split_size = int(seeds.shape[0]/5)\n",
    "\n",
    "fold_1 = seeds.iloc[0:split_size]\n",
    "fold_2 = seeds.iloc[split_size: 2*split_size]\n",
    "fold_3 = seeds.iloc[2*split_size: 3*split_size]\n",
    "fold_4 = seeds.iloc[3*split_size: 4*split_size]\n",
    "fold_5 = seeds.iloc[4*split_size: 5*split_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>P</th>\n",
       "      <th>C</th>\n",
       "      <th>L_Kern</th>\n",
       "      <th>W_Kern</th>\n",
       "      <th>Asy_Coeff</th>\n",
       "      <th>L_Kern_Grv</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142098</td>\n",
       "      <td>0.215462</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.304218</td>\n",
       "      <td>0.141702</td>\n",
       "      <td>-0.986152</td>\n",
       "      <td>-0.383577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>0.428515</td>\n",
       "      <td>-0.168625</td>\n",
       "      <td>0.197432</td>\n",
       "      <td>-1.788166</td>\n",
       "      <td>-0.922013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.192067</td>\n",
       "      <td>-0.360201</td>\n",
       "      <td>1.442383</td>\n",
       "      <td>-0.763637</td>\n",
       "      <td>0.208048</td>\n",
       "      <td>-0.667479</td>\n",
       "      <td>-1.189192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.347091</td>\n",
       "      <td>-0.475333</td>\n",
       "      <td>1.039381</td>\n",
       "      <td>-0.688978</td>\n",
       "      <td>0.319508</td>\n",
       "      <td>-0.960818</td>\n",
       "      <td>-1.229983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445257</td>\n",
       "      <td>0.330595</td>\n",
       "      <td>1.374509</td>\n",
       "      <td>0.066666</td>\n",
       "      <td>0.805159</td>\n",
       "      <td>-1.563495</td>\n",
       "      <td>-0.475356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         P         C    L_Kern    W_Kern  Asy_Coeff  L_Kern_Grv  Y\n",
       "0  0.142098  0.215462  0.000061  0.304218  0.141702  -0.986152   -0.383577  1\n",
       "1  0.011188  0.008224  0.428515 -0.168625  0.197432  -1.788166   -0.922013  1\n",
       "2 -0.192067 -0.360201  1.442383 -0.763637  0.208048  -0.667479   -1.189192  1\n",
       "3 -0.347091 -0.475333  1.039381 -0.688978  0.319508  -0.960818   -1.229983  1\n",
       "4  0.445257  0.330595  1.374509  0.066666  0.805159  -1.563495   -0.475356  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's Check how the dataset looks like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Problem 2.2 - Fill in the missing pieces for the k-NN implementation when you see a \"Step -\" prompt. Use the rest of the code to generate \n",
    "necessary metrics. \n",
    "\n",
    "Note: You can generate a completely different code if you find it easier, or use chunks of this code as you please.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "The Logic behind k-NN\n",
    "\n",
    "i. You want to learn k closest neighbors of any given point and assign to that point the most common class \n",
    "(either by vote or by computing an average) found in the neighborhood as defined by the closest k points.\n",
    "\n",
    "ii. Computationally, k-NN is called a Lazy method because there is actually no training involved, which means, that\n",
    "given a set of points with known classes (call them H), you can directly predict the class of a new point (t) given that\n",
    "you have H. As we discussed in (i), we take t, compute its distance from all points in H. Choose \"k\" points in H that\n",
    "aee closest to t. Take the most common class out of those k by voting or by computing an average. This common class is\n",
    "actually the predicted class for t\n",
    "\n",
    "iii. However, the question is how do we know what the value of \"k\" should be? The answer is that we do not know this \n",
    "apriori. Hence we have to do a grid search on a set of candidate values of \"k\" and use a resampling method such as \n",
    "5 fold CV or LOOCV to get a more robust answer\n",
    "\n",
    "iii. In the context of this problem, we consider both cases i.e. (a) 5 fold CV, (b) LOOCV\n",
    "\n",
    "(a) 5 fold CV - We create the 5 folds (Code provided). We make 5 passes through the data. In each pass, we reserve \n",
    "one fold and use the other 4 folds to predict the classes of the held out fold. We then use the prediction and the \n",
    "true label to compute an accuracy_score for that pass. At the end of 5 passes, we average out the accuracy_score and\n",
    "report is as the 5 fold cross validated accuracy_score\n",
    "\n",
    "(b) LOOCV - Same as above. Just that instead of 5 passes we have to make \"n\" (Size of the dataset) passes because in\n",
    "each pass we predict a [1 (true label matches predicted), 0(otherwise)] score for just one data point that is held out\n",
    "using the others to predict the class of the held out datapoint. In the end we count the number of 1's and divide by n \n",
    "to get the accuracy_score.\n",
    "\n",
    "\n",
    "Implementation Note:\n",
    "\n",
    "Remember that there is no training phase. For predicting the class of any point (from one of the five folds) we just\n",
    "look at its neighbourhood in the set of other 4 folds taken together.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function that finds Euclidean Distance Between Points. These two points are in the form of numpy lists.\n",
    "def pair_euclid_dist(a,b):\n",
    "    return numpy.linalg.norm(a-b, axis = 1)\n",
    "\n",
    "\n",
    "#We use the trg dataset to predict the classes of each point of the test set.\n",
    "#Here both trg and test are pandas dataframes\n",
    "def knn_predict(trg, tst, k):\n",
    "    \n",
    "    #Initialize a counter\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    #Iterate over all datapoints in tst. One point at a time taken into consideration.\n",
    "    for index, row in tst.iterrows(): \n",
    "            '''\n",
    "                Step - Use the pair_euclid_dist function from above to calculate pair wise distances from\n",
    "                every point in trg for the point under consideration\n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "                Step - sort the distances above and take the k points in trg that have the least distances\n",
    "                from the point under consideration.\n",
    "            '''\n",
    "    \n",
    "            '''\n",
    "                Step - For the k selected points, take each of their classes and take a majority vote.\n",
    "                This majority vote is your prediction for the current datapoint under consideration from tst.\n",
    "                Make sure you have a mechanism that solves for ties. \n",
    "            '''\n",
    "        \n",
    "            '''\n",
    "                Step - if this prediction == true label of the tst data point under consideration, \n",
    "                increment the counter above i.e. correct_predictions += 1\n",
    "            '''\n",
    "    \n",
    "    #Report the Average Accuracy over tst.\n",
    "    \n",
    "    return correct_predictions/tst.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all k's that we want to try for k-NN. We don't know k apriori, so we need to grid search.\n",
    "k_try = [1,5,10,15]\n",
    "\n",
    "\n",
    "#Function that estimates Accuracy of the k-nn classifier using 5 fold Cross Validation\n",
    "def knn_cv(fold_1,fold_2,fold_3,fold_4,fold_5):\n",
    "    '''\n",
    "    This function iterates through the various values of k and prints out the average \n",
    "    accuracy using CV \n",
    "    '''\n",
    "    #Iterate over all values of the tuning Parameter\n",
    "    for trial in k_try:\n",
    "        #Try k-nn for each fold and Average the Classification Accuracy\n",
    "        \n",
    "        '''\n",
    "        Step - Find a way to call knn_predict() function above such that each time your parameter\n",
    "        tst is one of the five folds and trg are the other four folds stacked on top of each together.\n",
    "        '''\n",
    "        \n",
    "        tst_acc_f1 = knn_predict(...)\n",
    "        tst_acc_f2 = knn_predict(...)\n",
    "        tst_acc_f3 = knn_predict(...)\n",
    "        tst_acc_f4 = knn_predict(...)\n",
    "        tst_acc_f5 = knn_predict(...)\n",
    "        print('CV - 5 Accuracy for k_try = ',trial, ' is: ', np.mean([tst_acc_f1,\n",
    "                                                                      tst_acc_f2,\n",
    "                                                                      tst_acc_f3,\n",
    "                                                                      tst_acc_f4,\n",
    "                                                                      tst_acc_f5]))\n",
    "\n",
    "        \n",
    "#Function that estimates Accuracy of the k-nn classifier using Leave One out Validation\n",
    "def knn_loocv(full_set):\n",
    "    \n",
    "    #Iterate over all values of the tuning Parameter\n",
    "    for trial in k_try:\n",
    "        \n",
    "        '''\n",
    "           Step - Fill in the code here to do LOOCV. Idea is the same. Just that tst has size 1. trg is all\n",
    "           other points in dataset seeds.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "          Step - Remember, you have to report the average accuracy over all datapoints.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "          Step - Print appropriately.\n",
    "        '''\n",
    "        print('LOOCV Accuracy for k_try = ',trial, ' is: ', .......)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit KNN with CV = 5   \n",
    "knn_cv(fold_1,fold_2,fold_3,fold_4,fold_5)\n",
    "\n",
    "'''        \n",
    "Step - Finally you have to report the best \"k\" i.e. one that produces the highest mean accuracy\n",
    "'''\n",
    "\n",
    "#Fit KNN with LOOCV      \n",
    "knn_loocv(seeds) #Remember seeds is the name of our full dataset.\n",
    "\n",
    "#Plotting Test Errors from both 5 fold CV and LOOCV\n",
    "\n",
    "'''\n",
    "Step\n",
    "Line plot of errors for each k for 5 fold CV\n",
    "Line plot of errors for each k for LOOCV\n",
    "\n",
    "Remark:\n",
    "\n",
    "Make sure the plot is properly labeled.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Problem 2.3 - The following example shows steps to call an inbuilt classifier (Random Forest) from Scikit Learn. \n",
    "Choose two other classifiers from scikit-learn to solve this problem. The sequence of steps will be roughly the same.\n",
    "\n",
    "Note : Do not use Random Forests (because we want you to learn to fit your own classifiers). \n",
    "To see a list of available list of classifiers, run the code block below\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres duque\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AdaBoostClassifier', <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>)\n",
      "('BaggingClassifier', <class 'sklearn.ensemble._bagging.BaggingClassifier'>)\n",
      "('BernoulliNB', <class 'sklearn.naive_bayes.BernoulliNB'>)\n",
      "('CalibratedClassifierCV', <class 'sklearn.calibration.CalibratedClassifierCV'>)\n",
      "('CategoricalNB', <class 'sklearn.naive_bayes.CategoricalNB'>)\n",
      "('CheckingClassifier', <class 'sklearn.utils._mocking.CheckingClassifier'>)\n",
      "('ClassifierChain', <class 'sklearn.multioutput.ClassifierChain'>)\n",
      "('ComplementNB', <class 'sklearn.naive_bayes.ComplementNB'>)\n",
      "('DecisionTreeClassifier', <class 'sklearn.tree._classes.DecisionTreeClassifier'>)\n",
      "('DummyClassifier', <class 'sklearn.dummy.DummyClassifier'>)\n",
      "('ExtraTreeClassifier', <class 'sklearn.tree._classes.ExtraTreeClassifier'>)\n",
      "('ExtraTreesClassifier', <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>)\n",
      "('GaussianNB', <class 'sklearn.naive_bayes.GaussianNB'>)\n",
      "('GaussianProcessClassifier', <class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>)\n",
      "('GradientBoostingClassifier', <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>)\n",
      "('HistGradientBoostingClassifier', <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>)\n",
      "('KNeighborsClassifier', <class 'sklearn.neighbors._classification.KNeighborsClassifier'>)\n",
      "('LabelPropagation', <class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>)\n",
      "('LabelSpreading', <class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>)\n",
      "('LinearDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>)\n",
      "('LinearSVC', <class 'sklearn.svm._classes.LinearSVC'>)\n",
      "('LogisticRegression', <class 'sklearn.linear_model._logistic.LogisticRegression'>)\n",
      "('LogisticRegressionCV', <class 'sklearn.linear_model._logistic.LogisticRegressionCV'>)\n",
      "('MLPClassifier', <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>)\n",
      "('MultiOutputClassifier', <class 'sklearn.multioutput.MultiOutputClassifier'>)\n",
      "('MultinomialNB', <class 'sklearn.naive_bayes.MultinomialNB'>)\n",
      "('NearestCentroid', <class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>)\n",
      "('NuSVC', <class 'sklearn.svm._classes.NuSVC'>)\n",
      "('OneVsOneClassifier', <class 'sklearn.multiclass.OneVsOneClassifier'>)\n",
      "('OneVsRestClassifier', <class 'sklearn.multiclass.OneVsRestClassifier'>)\n",
      "('OutputCodeClassifier', <class 'sklearn.multiclass.OutputCodeClassifier'>)\n",
      "('PassiveAggressiveClassifier', <class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>)\n",
      "('Perceptron', <class 'sklearn.linear_model._perceptron.Perceptron'>)\n",
      "('QuadraticDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>)\n",
      "('RadiusNeighborsClassifier', <class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>)\n",
      "('RandomForestClassifier', <class 'sklearn.ensemble._forest.RandomForestClassifier'>)\n",
      "('RidgeClassifier', <class 'sklearn.linear_model._ridge.RidgeClassifier'>)\n",
      "('RidgeClassifierCV', <class 'sklearn.linear_model._ridge.RidgeClassifierCV'>)\n",
      "('SGDClassifier', <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>)\n",
      "('SVC', <class 'sklearn.svm._classes.SVC'>)\n",
      "('StackingClassifier', <class 'sklearn.ensemble._stacking.StackingClassifier'>)\n",
      "('VotingClassifier', <class 'sklearn.ensemble._voting.VotingClassifier'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres duque\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Prints a list of available inbuilt classifiers'''\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.utils.testing import all_estimators\n",
    "classifiers=[est for est in all_estimators() if issubclass(est[1], ClassifierMixin)]\n",
    "for classifier in classifiers:\n",
    "    print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u6022720/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF - Best Params =  {'n_estimators': 50, 'max_features': 'log2'}\n",
      "CV - 5 : Accuracy Score =  0.9455782312925171\n",
      "Test Set Accuracy =  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "'''Random Forest'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Define X, y\n",
    "X = seeds[['A', 'P', 'C', 'L_Kern','W_Kern', 'Asy_Coeff','L_Kern_Grv']]\n",
    "y = seeds['Y']\n",
    "\n",
    "#Test Train Split - Using the Built in Method.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#Initialize the RF Classifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "#Define Parameter Grid to perform Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#Initiate Grid Search\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "\n",
    "#Fit Model with Grid Search\n",
    "CV_rfc.fit(X_train,y_train)\n",
    "\n",
    "#Training Accuracy\n",
    "print('RF - Best Params = ',CV_rfc.best_params_)\n",
    "print('CV - 5 : Accuracy Score = ',CV_rfc.best_score_ )\n",
    "\n",
    "#Test Accuracy\n",
    "print('Test Set Accuracy = ', CV_rfc.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
