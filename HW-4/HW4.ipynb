{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can start using the same code as in HW3. Some things to consider:\n",
    "\n",
    "- Now we are using the skorch library which allows us to do parameter tunning in the same way as for\n",
    "other classifiers in sklearn. \n",
    "- The code to add L1 regularization is already implemented.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import packages \n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device configuration - If you have CUDA configured, you must use it. Try training with CPU and observe what happens\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "'''\n",
    "Step - Import Mnist training and testing data sets as we did for HW3. But do not use Dataloader, \n",
    "skorch will do that step for us.\n",
    "\n",
    "Step - Normalize the data so the pixel values are between 0 and 1. \n",
    "     - Make sure the shape of the data tensor is not (n_observations, 28, 28), but (n_observations, 784)\n",
    "     - Also make sure the data is type float and the targets type integer\n",
    "     - Name the data X_train and X_test and the labels y_train and y_test\n",
    "     \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgQGKYQIlrFt"
   },
   "outputs": [],
   "source": [
    "#Adding L1 regularization\n",
    "\n",
    "from skorch import NeuralNet\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Step - Fill in the Neural Network here. Use the same arquitecture as in HW3, but now add a new hidden layer.\n",
    "    and dropout\n",
    "    Remember to use ReLU activations in the hidden layers. \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pass\n",
    "\n",
    "class RegularizedNet(NeuralNet):\n",
    "    \n",
    "    ''''''\n",
    "    \n",
    "    def __init__(self, *args, lambda1=0.01, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda1 = lambda1\n",
    "    \n",
    "    ''' *** - What is the following method doing? Explain in detail in the main pdf ***'''\n",
    "    \n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        loss = super().get_loss(y_pred, y_true, X=X, training=training)\n",
    "        loss += self.lambda1 * sum([w.abs().sum() for w in self.module_.parameters()])\n",
    "        return loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgQGKYQIlrFt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here we define the RegularizedNet. Make sure you use nn.NLLLoss. Thus, you have to use a correct last activation\n",
    "in the forward method of your network\n",
    "\n",
    "We can especify different parameters as learning rate (lr), our optimizar (start with standard SGD, in 5.3 we will\n",
    "try another ones), batch size etc.\n",
    "To define the arquitecture parameters for MyModule write them as module__<name of your parameter> = ....\n",
    "\n",
    "Since we have to train it first with L2 regularization lambda1 should be equal to 0\n",
    "'''\n",
    "new_net = RegularizedNet(module=MyModule, criterion=torch.nn.NLLLoss, \n",
    "                        optimizer=..., lr = 0.001, lambda1 = 0,  module__dropout = ...,\n",
    "                        optimizer__weight_decay = ...)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gb6yTL_ulzD6"
   },
   "outputs": [],
   "source": [
    "'''Step - train the network'''\n",
    "\n",
    "new_net.fit(X_train, y_train)\n",
    "y_pred_probs = new_net.predict(X_test)\n",
    "'''\n",
    "Look how your loss is going down as well as the validation accuracy is increasing \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFN1PQ50lzGi"
   },
   "outputs": [],
   "source": [
    "'''Step - Predict for the test set and print the final accuracy score, your validation accuracy obtained in the previous\n",
    "cell should be similar to the accuracy in the test set\n",
    "'''\n",
    "y_pred = new_net.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXbcnqA7lzIi"
   },
   "outputs": [],
   "source": [
    "'''The idea is that you should get more than 98% of accuracy, so try different parameters as requested in the main pdf\n",
    "The fit method is already showing you a validation error which can be used to compare between different parameters.\n",
    "\n",
    "for the final submission leave the best parameters in your RegularizedNet(...)\n",
    "'''\n",
    "'''\n",
    "Instead of doing it manually skorch allows us to use GridSearchCV from sklearn \n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "'''\n",
    "Step - define a grid with some parameters that you consider may give you good results and \n",
    "the code will do the rest for you\n",
    "\n",
    "* Especially take into account the parameters we are asking for to tune:  \n",
    "Learning rate, regularization parameter, and the number of nodes\n",
    "\n",
    "'''\n",
    "\n",
    "grid = {\n",
    "    'lr': [0.001, 0.1,...],\n",
    "    'other parameter': [....]\n",
    "}\n",
    "\n",
    "'''\n",
    "Important that you keep refit = True\n",
    "'''\n",
    "gs = GridSearchCV(new_net, grid, refit=True, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "'''\n",
    "Finally fit\n",
    "'''\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "#Report Best Parameters\n",
    "print(gs.best_score_, gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step - Now we are going to train the network with L1 regularization instead of L2 and dropout,\n",
    "we are going to create a new network with a lambda1 parameter different than 0\n",
    "- Keep the rest of the parameters you used in the previous network but dropout and L2 parameters are 0 \n",
    "'''\n",
    "\n",
    "new_net_l1 = RegularizedNet(module=MyModule, criterion=torch.nn.NLLLoss, \n",
    "                        optimizer= ..., lr = 0.001, lambda1 = ...,  module__dropout = 0,\n",
    "                        optimizer__weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Knz4An8-lvAt"
   },
   "outputs": [],
   "source": [
    "#Refer to https://skorch.readthedocs.io/en/stable/user/save_load.html\n",
    "\n",
    "import pickle\n",
    "\n",
    "#Transfer Learning - \n",
    "#The following code will transfer the weights from L2 trained networks to initialize the new network before L1 training\n",
    "\n",
    "'''\n",
    "\n",
    "Notes - I assumed you have trained your L2 network using Skorch's NeuralNetClassifier\n",
    "        I assume your trained model object is called \"new_net\"\n",
    "\n",
    "'''\n",
    "\n",
    "#Step - 1 - Save weights from L2 network\n",
    "\n",
    "new_net.save_params(f_params='some-file.pkl') #This comes after net.fit(). You are saving the model weights in a pickle\n",
    "\n",
    "\n",
    "#Step - 2\n",
    "\n",
    "new_net_l1.initialize()\n",
    "new_net_l1.load_params(f_params='some-file.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step train the network with the weights transfered from new_net, and perform grid search for the lambda1 parameter\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step train the network with default initialization parameters\n",
    "we can simply initialize with the same code as before (make sure to use the same parameters)\n",
    "\n",
    "perform grid search for the lambda1 parameter as in the previous cell\n",
    "'''\n",
    "\n",
    "new_net_l1 = RegularizedNet(module=MyModule, criterion=torch.nn.NLLLoss, \n",
    "                        optimizer= ..., lr = 0.001, lambda1 = ...,  module__dropout = 0,\n",
    "                        optimizer__weight_decay = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keeping all the parameters for which you have got the best results before\n",
    "try different optimizers.\n",
    "\n",
    "Basically create the same new_net_l1 or new_net but train it with the requested optimizers in the pdf\n",
    "\n",
    "GridSeacrh is not required but you can do it if you want for the different parameters of the optimizers\n",
    "\n",
    "Notice you already train it with SGD in the previous problems\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam (look how I defined optimizer)\n",
    "\n",
    "new_net_l1 = RegularizedNet(module=MyModule, criterion=torch.nn.NLLLoss, \n",
    "                        optimizer= torch.optim.Adam, lr = 0.001, lambda1 = ...,  module__dropout = 0,\n",
    "                        optimizer__weight_decay = 0)\n",
    "\n",
    "'''\n",
    "-Step now fit it and print the accuracy as in problem 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
